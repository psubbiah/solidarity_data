[https://zenodo.org/account/settings/github/repository/psubbiah/solidarity\_data][1]
# Textual Analysis of UK Solidarity Movements with the Bolivarian Revolution in Venezuela

Created by P. Subbiah in 2020.

## Introduction
Textual Analysis insights into groups supportive of the Maduro Government in Venezuela.

The Hands off Venezuela Campaign, is a campaign that, in their own words, stand: 
- “in solidarity with the Bolivarian Revolution”; 
- are “opposed to imperialist intervention in Venezuela”
- are “building direct links with the revolutionary trade unions in Venezuela.”

In this project, I analyse the discourse of ‘solidarity’ by documenting the online output of the Hands Off Venezuela Campaign, and using several computational methods.

## Data

The data used for the analysis was scraped by building a custom ‘Scrapy spider’, which you can find under the folder “solidarity.” The spider scraped all the articles published on the Hands Off Venezuela website, that date back to 2003. The spider is written in Python. You can clone and run the spider by typing the following commands in your console:

First install Scrapy:

	pip install Scrapy 

Run the spider, from inside the solidarity directory, by typing:

	scrapy crawl solidarity - o solidarity.csv 

Again, make sure you are in the solidarity directory or Scrapy will not be able to find the files it needs to run. This particular command will save the output of the spider onto a .csv file, that you will then use to run the analyses.


## Scripts

I have separated the analysis into two sections: 
1. Section 1: 
	1. Prepares the data for analysis
	2. Performs sentiment analysis using the NRC Lexicon from the ‘tidytext’ package
	3. Filters for superfluous word that do not add meaning
	4. Generates a word cloud (to show word frequencies)
	5. Generates a plot that represents a Markov chain, that is, plots the words as they appear in relation to one another (this is also known as a ‘bigram’ network analysis

To run this section from your console:

`Rscript hov_sentiments.R input_path output_path`

Where the first argument is the path to the .csv generated by the spider, and the second argument is the path to the directory where you would like to save the plots generated.

2. Section 2: 
	1. Prepares the data for the LDA and RAKE machine learning algorithms using the ‘udpipe’ package (this includes annotating all the words with udpipe’s English model) 
	2. Uses RAKE to calculate top nouns, adjectives and verbs
	3. Uses RAKE to calculate most prominent ‘Noun phrases’ according to the algorithm
	4. Uses the LDA to generate a group of ‘topics’ that ‘represent’ the group of texts, using the ‘topicmodels’ package. (I generate two models, preassigning 4 topics for the first model, and 10 topics for the second). I note that this graph requires qualitative interpretation. 

To run this section from your console:

`Rscript hov_LDA.R input_path output_path`

Where the first argument is the path to the .csv generated by the spider, and the second argument is the path to the directory where you would like to save the plots generated.

## Findings on Sentiment Analysis and the NRC Lexicon

The NRC lexicon classifies the words ‘socialism’ and ‘revolution’ under the emotions of ‘anger’, ‘fear’, ‘disgust’, ‘sadness.’ This was a mistake for the group being studied here who are of the extreme left, and on the contrary to what the algorithm is suggesting, use these words in a very positive light. This comes to show that applying these sentiment analysis algorithms without a deeper understanding of the particular groups we are applying these sentiment analyses too, can be highly misleading. 

For an explanation of how the NRC lexicon was built using Amazon’s Mechanical Turk and its methodology, visit the authors’ website [here][2].

## References

This project is indebted to many tutorials:
- For more on the RAKE algorithm, please see [this post][3] by Abdul Majed Raja
- For more on the LDA algorithm and how it can be used, see [this post][4] by Debbie Liske.
- For more on the qualitative and field knowledge that is required to interpret LDA topic model outputs, see [this post][5] by Dr. Benjamin Soltoff. 
- For more on the assumptions and math behind the algorithm, developed by Prof. David M. Blei in 2003, see [this post][6] where Ankur Thomas explains some of ideas behind the model.
- For more on bigram-network analysis please see [this chapter][7] in Julia Silge and David Robinson’s influential book _Text Mining with R._

[1]:	https://zenodo.org/account/settings/github/repository/psubbiah/solidarity_data
[2]:	https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm
[3]:	https://datascienceplus.com/introducing-udpipe-for-easy-natural-language-processing-in-r/
[4]:	https://www.datacamp.com/community/tutorials/ML-NLP-lyric-analysis#model_four
[5]:	%20https://cfss.uchicago.edu/notes/topic-modeling/
[6]:	https://medium.com/analytics-vidhya/topic-modeling-using-lda-and-gibbs-sampling-explained-49d49b3d1045
[7]:	https://www.tidytextmining.com/ngrams.html